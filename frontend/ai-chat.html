<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>PitchLense - AI Conversation Assistant</title>
  <link rel="icon" type="image/svg+xml" href="/static/logo.svg" />
  <meta name="description" content="AI-powered conversational follow-up with voice assistance" />
  
  <!-- Tailwind Play CDN with custom theme -->
  <script src="https://cdn.tailwindcss.com"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            background: '#1E1E21',
            surface: '#2E3137',
            muted: '#2E3137',
            text: '#ffffff',
            secondary: '#cfd4dd',
            accent: '#f1d85b',
            accent2: '#78e6d0'
          },
          fontFamily: {
            sans: ['Inter', 'system-ui', 'Segoe UI', 'Roboto', 'Helvetica', 'Arial', 'sans-serif']
          },
          boxShadow: {
            card: '0 10px 30px rgba(0,0,0,0.35)'
          }
        }
      }
    };
  </script>
  
  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800;900&display=swap" rel="stylesheet">
  
  <!-- Base styles -->
  <link rel="stylesheet" href="./styles.css" />
  
  <!-- SweetAlert2 -->
  <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11"></script>
  
  <!-- Gemini Live API will be loaded dynamically to avoid CSP issues -->
</head>
<body class="bg-background text-text font-sans antialiased selection:bg-accent selection:text-white h-screen overflow-hidden">
  <div class="h-screen flex flex-col">
    <!-- Header -->
    <header class="liquid-glass border-b border-white/10 px-6 py-4 flex-shrink-0">
      <div class="flex items-center justify-between">
        <div class="flex items-center gap-3">
          <img src="/static/logo.svg" class="w-8 h-8" alt="PitchLense">
          <h1 class="text-xl font-bold text-white">PitchLense AI Conversation</h1>
        </div>
        <div class="flex items-center gap-4 text-sm">
          <a href="/profile.html" class="text-white/60 hover:text-accent transition-colors" title="Profile">
            Profile
          </a>
          <a href="#" class="text-white/60 hover:text-accent transition-colors" title="Drop email to connectamanulla@gmail.com">
            Need help?
          </a>
        </div>
      </div>
    </header>

    <!-- Main Content -->
    <main class="flex-1 flex min-h-0">
      <!-- Video Section (70%) -->
      <div class="w-3/5 p-6 min-w-0">
        <div class="h-full flex flex-col">
          <!-- Video Container -->
          <div class="flex-1 liquid-glass-card p-6 mb-4 min-h-0">
            <div class="h-full flex flex-col">
              <!-- Video Preview -->
              <div class="flex-1 bg-black rounded-lg mb-4 flex items-center justify-center relative overflow-hidden">
                <video id="videoPreview" autoplay muted class="w-full h-full object-contain hidden"></video>
                <div id="videoPlaceholder" class="text-center text-white/60">
                  <svg class="w-16 h-16 mx-auto mb-4 text-white/40" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M15 10l4.553-2.276A1 1 0 0121 8.618v6.764a1 1 0 01-1.447.894L15 14M5 18h8a2 2 0 002-2V8a2 2 0 00-2-2H5a2 2 0 00-2 2v8a2 2 0 002 2z"></path>
                  </svg>
                  <p class="text-lg">Camera Preview</p>
                  <p class="text-sm">Select a camera to start</p>
                </div>
                
                <!-- AI Speaking Indicator -->
                <div id="aiSpeakingIndicator" class="absolute top-4 right-4 hidden">
                  <div class="bg-black/70 rounded-full p-3">
                    <div class="relative">
                      <!-- Voice Visualization Circles -->
                      <div class="relative w-12 h-12 flex items-center justify-center">
                        <!-- Outer pulsing circle -->
                        <div class="absolute w-12 h-12 rounded-full border-2 border-accent/30 animate-ping"></div>
                        <!-- Middle circle -->
                        <div class="absolute w-8 h-8 rounded-full border-2 border-accent/50 animate-pulse"></div>
                        <!-- Inner circle -->
                        <div class="absolute w-4 h-4 rounded-full bg-accent animate-bounce"></div>
                        <!-- AI Icon -->
                        <div class="absolute inset-0 flex items-center justify-center z-10">
                          <svg class="w-3 h-3 text-accent" fill="currentColor" viewBox="0 0 24 24">
                            <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm-2 15l-5-5 1.41-1.41L10 14.17l7.59-7.59L19 8l-9 9z"/>
                          </svg>
                        </div>
                      </div>
                    </div>
                  </div>
                </div>
                
                <!-- Audio Level Indicator -->
                <div id="audioLevelIndicator" class="absolute bottom-4 left-4 hidden">
                  <div class="bg-black/70 rounded-lg p-2">
                    <div class="flex items-center gap-2">
                      <svg class="w-4 h-4 text-white" fill="currentColor" viewBox="0 0 24 24">
                        <path d="M3 9v6h4l5 5V4L7 9H3zm13.5 3c0-1.77-1.02-3.29-2.5-4.03v8.05c1.48-.73 2.5-2.25 2.5-4.02zM14 3.23v2.06c2.89.86 5 3.54 5 6.71s-2.11 5.85-5 6.71v2.06c4.01-.91 7-4.49 7-8.77s-2.99-7.86-7-8.77z"/>
                      </svg>
                      <div class="flex items-center gap-1">
                        <div id="audioBar1" class="w-1 h-2 bg-green-400 rounded"></div>
                        <div id="audioBar2" class="w-1 h-3 bg-green-400 rounded"></div>
                        <div id="audioBar3" class="w-1 h-4 bg-green-400 rounded"></div>
                        <div id="audioBar4" class="w-1 h-5 bg-green-400 rounded"></div>
                        <div id="audioBar5" class="w-1 h-6 bg-green-400 rounded"></div>
                      </div>
                    </div>
                  </div>
                </div>
              </div>

              <!-- Start Conversation Section -->
              <div id="startConversationSection" class="text-center space-y-6">
                <div class="space-y-4">
                  <div class="w-20 h-20 bg-accent/20 rounded-full flex items-center justify-center mx-auto">
                    <svg class="w-10 h-10 text-accent" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                      <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 12h.01M12 12h.01M16 12h.01M21 12c0 4.418-4.03 8-9 8a9.863 9.863 0 01-4.255-.949L3 20l1.395-3.72C3.512 15.042 3 13.574 3 12c0-4.418 4.03-8 9-8s9 3.582 9 8z"></path>
                    </svg>
                  </div>
                  <h2 class="text-2xl font-bold text-white">Start AI Conversation</h2>
                  <p class="text-white/70 max-w-md mx-auto leading-relaxed">
                    I'll have a natural conversation with you and ask about the topics we need to cover. 
                    Just speak naturally and I'll guide our discussion.
                  </p>
                </div>
                
                <!-- Instructions -->
                <div class="bg-[#1E1E21] border border-white/10 rounded-lg p-4 text-left max-w-lg mx-auto">
                  <h3 class="text-white font-semibold mb-3 flex items-center gap-2">
                    <svg class="w-5 h-5 text-accent" fill="currentColor" viewBox="0 0 24 24">
                      <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm-2 15l-5-5 1.41-1.41L10 14.17l7.59-7.59L19 8l-9 9z"/>
                    </svg>
                    How it works:
                  </h3>
                  <ul class="space-y-2 text-sm text-white/70">
                    <li class="flex items-start gap-2">
                      <span class="w-1.5 h-1.5 bg-accent rounded-full mt-2 flex-shrink-0"></span>
                      <span>I'll start our conversation naturally</span>
                    </li>
                    <li class="flex items-start gap-2">
                      <span class="w-1.5 h-1.5 bg-accent rounded-full mt-2 flex-shrink-0"></span>
                      <span>Speak naturally - no need to rush</span>
                    </li>
                    <li class="flex items-start gap-2">
                      <span class="w-1.5 h-1.5 bg-accent rounded-full mt-2 flex-shrink-0"></span>
                      <span>I'll ask follow-up questions as needed</span>
                    </li>
                    <li class="flex items-start gap-2">
                      <span class="w-1.5 h-1.5 bg-accent rounded-full mt-2 flex-shrink-0"></span>
                      <span>Take your time to think and respond</span>
                    </li>
                  </ul>
                </div>

                <!-- Device Selection -->
                <div class="grid grid-cols-2 gap-4 max-w-lg mx-auto">
                  <!-- Camera Selection -->
                  <div class="flex flex-col gap-2">
                    <label class="text-white font-medium text-sm">Camera:</label>
                    <select id="cameraSelect" class="bg-[#1E1E21] border border-white/20 rounded-lg px-3 py-2 text-white focus:outline-none focus:border-accent">
                      <option value="">Select Camera...</option>
                    </select>
                  </div>

                  <!-- Audio Selection -->
                  <div class="flex flex-col gap-2">
                    <label class="text-white font-medium text-sm">Audio:</label>
                    <select id="audioSelect" class="bg-[#1E1E21] border border-white/20 rounded-lg px-3 py-2 text-white focus:outline-none focus:border-accent">
                      <option value="">Select Audio Device...</option>
                    </select>
                  </div>
                </div>

                <!-- Desktop Audio Setup -->
                <div id="desktopAudioSetup" class="bg-blue-900/20 border border-blue-500/30 rounded-lg p-4 mb-4">
                  <div class="flex items-start gap-3">
                    <div class="w-6 h-6 bg-blue-500 rounded-full flex items-center justify-center flex-shrink-0 mt-0.5">
                      <svg class="w-3 h-3 text-white" fill="currentColor" viewBox="0 0 24 24">
                        <path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm-2 15l-5-5 1.41-1.41L10 14.17l7.59-7.59L19 8l-9 9z"/>
                      </svg>
                    </div>
                    <div class="flex-1">
                      <h4 class="text-white font-medium mb-2">Desktop Audio Capture (Optional)</h4>
                      <p class="text-white/70 text-sm mb-3">To record the AI's voice responses, enable desktop audio capture. This is optional - you can still proceed with just your microphone.</p>
                      <div class="flex items-center gap-3">
                        <button id="enableDesktopAudio" class="px-4 py-2 bg-blue-600 text-white rounded-lg hover:bg-blue-700 transition-colors text-sm font-medium">
                          Enable Desktop Audio
                        </button>
                        <button id="skipDesktopAudio" class="px-4 py-2 bg-gray-600 text-white rounded-lg hover:bg-gray-700 transition-colors text-sm font-medium">
                          Skip (Microphone Only)
                        </button>
                      </div>
                      <div id="desktopAudioStatus" class="mt-2 text-sm text-white/60"></div>
                    </div>
                  </div>
                </div>

                <!-- Start Button -->
                <button id="startConversation" class="px-8 py-4 bg-accent text-[#1E1E21] rounded-lg hover:bg-accent/80 transition-colors font-medium text-lg disabled:opacity-50 disabled:cursor-not-allowed" disabled>
                  <svg class="w-6 h-6 inline mr-2" fill="currentColor" viewBox="0 0 24 24">
                    <path d="M8 5v14l11-7z"/>
                  </svg>
                  Start Conversation
                </button>
              </div>

              <!-- Recording Controls (Hidden initially) -->
              <div id="recordingControls" class="hidden">
                <!-- AI Status Display -->
                <div id="aiStatusDisplay" class="bg-[#1E1E21] border border-white/10 rounded-lg p-4 mb-4">
                  <div class="flex items-center gap-3">
                    <div class="w-3 h-3 bg-accent rounded-full animate-pulse"></div>
                    <span id="aiStatusText" class="text-white font-medium">AI Assistant is speaking...</span>
                  </div>
                </div>

                <!-- Recording Status -->
                <div id="recordingStatus" class="text-center text-white/70 mb-4">
                  <div class="flex items-center justify-center gap-2">
                    <div class="w-3 h-3 bg-red-500 rounded-full animate-pulse"></div>
                    <span>Recording conversation...</span>
                    <span id="recordingTime">00:00</span>
                  </div>
                </div>
              </div>

              <!-- Upload Progress -->
              <div id="uploadProgress" class="hidden">
                <div class="bg-[#1E1E21] border border-white/20 rounded-lg p-4">
                  <div class="flex items-center justify-between mb-2">
                    <span class="text-white font-medium">Uploading Video...</span>
                    <span id="uploadPercentage" class="text-white/60">0%</span>
                  </div>
                  <div class="w-full bg-white/10 rounded-full h-2">
                    <div id="uploadBar" class="bg-accent h-2 rounded-full transition-all duration-300" style="width: 0%"></div>
                  </div>
                </div>
              </div>

              <!-- Processing Loader -->
              <div id="processingLoader" class="hidden">
                <div class="bg-[#1E1E21] border border-white/20 rounded-lg p-6 text-center">
                  <div class="flex items-center justify-center mb-4">
                    <div class="w-8 h-8 border-2 border-accent border-t-transparent rounded-full animate-spin mr-3"></div>
                    <span class="text-white font-medium">Processing Conversation...</span>
                  </div>
                  <p class="text-white/70 text-sm mb-2">Analyzing video with AI</p>
                  <p class="text-white/60 text-xs">Generating transcript and extracting insights</p>
                </div>
              </div>

              <!-- Completion Message -->
              <div id="completionMessage" class="hidden">
                <div class="bg-green-900/20 border border-green-500/30 rounded-lg p-6 text-center">
                  <div class="w-16 h-16 bg-green-500 rounded-full flex items-center justify-center mx-auto mb-4">
                    <svg class="w-8 h-8 text-white" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                      <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 13l4 4L19 7"></path>
                    </svg>
                  </div>
                  <h3 class="text-xl font-semibold text-white mb-2">Conversation Completed Successfully!</h3>
                  <p class="text-white/70 mb-4">Your conversation has been processed and analyzed. The transcript and insights have been saved.</p>
                  <div class="space-y-3">
                    <p class="text-white/60 text-sm">You can now safely close this tab:</p>
                    <div class="flex flex-col sm:flex-row gap-2 justify-center">
                      <button onclick="closeTab()" class="px-6 py-2 bg-green-600 text-white rounded-lg hover:bg-green-700 transition-colors font-medium">
                        Try Auto Close
                      </button>
                      <button onclick="showCloseInstructions()" class="px-6 py-2 bg-blue-600 text-white rounded-lg hover:bg-blue-700 transition-colors font-medium">
                        Manual Close Instructions
                      </button>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>

      <!-- Questions Section (30%) -->
      <div class="w-2/5 liquid-glass-card p-6 flex flex-col min-w-0">
        <h2 class="text-lg font-semibold text-white mb-4 flex-shrink-0">Topics to Cover</h2>
        <div id="topicsContainer" class="space-y-3 overflow-y-auto flex-1 min-h-0">
          <!-- Topics will be loaded here -->
        </div>
      </div>
    </main>

    <!-- Bottom Dock (Hidden initially) -->
    <div id="bottomDock" class="fixed bottom-6 left-1/2 transform -translate-x-1/2 hidden">
      <div class="bg-surface/95 backdrop-blur-sm border border-white/20 rounded-full px-6 py-3 flex items-center gap-4 shadow-lg">
        <!-- Restart Button -->
        <button id="restartRecording" class="p-3 bg-orange-600 text-white rounded-full hover:bg-orange-700 transition-colors group" title="Restart Conversation">
          <svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 4v5h.582m15.356 2A8.001 8.001 0 004.582 9m0 0H9m11 11v-5h-.581m0 0a8.003 8.003 0 01-15.357-2m15.357 2H15"></path>
          </svg>
        </button>
        
        <!-- Submit Button -->
        <button id="submitConversation" class="p-3 bg-green-600 text-white rounded-full hover:bg-green-700 transition-colors group" title="Submit Conversation">
          <svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 19l9 2-9-18-9 18 9-2zm0 0v-8"></path>
          </svg>
        </button>
      </div>
    </div>
  </div>

  <script>
    // Global variables
    let mediaRecorder;
    let recordedChunks = [];
    let recordingStartTime;
    let recordingTimer;
    let stream;
    let audioContext;
    let analyser;
    let microphone;
    let animationFrame;
    let recordedVideoBlob;
    let currentStream;
    let isAISpeaking = false;
    let currentSpeech = null;
    let geminiSession = null;
    let isRecording = false;
    let conversationStarted = false;
    let desktopAudioStream = null;
    let desktopAudioEnabled = false;

    // DOM elements
    const videoPreview = document.getElementById('videoPreview');
    const videoPlaceholder = document.getElementById('videoPlaceholder');
    const cameraSelect = document.getElementById('cameraSelect');
    const audioSelect = document.getElementById('audioSelect');
    const startConversation = document.getElementById('startConversation');
    const recordingStatus = document.getElementById('recordingStatus');
    const recordingTime = document.getElementById('recordingTime');
    const uploadProgress = document.getElementById('uploadProgress');
    const uploadBar = document.getElementById('uploadBar');
    const uploadPercentage = document.getElementById('uploadPercentage');
    const processingLoader = document.getElementById('processingLoader');
    const completionMessage = document.getElementById('completionMessage');
    const topicsContainer = document.getElementById('topicsContainer');
    const aiSpeakingIndicator = document.getElementById('aiSpeakingIndicator');
    const aiStatusDisplay = document.getElementById('aiStatusDisplay');
    const aiStatusText = document.getElementById('aiStatusText');
    const startConversationSection = document.getElementById('startConversationSection');
    const recordingControls = document.getElementById('recordingControls');
    const bottomDock = document.getElementById('bottomDock');
    const restartRecording = document.getElementById('restartRecording');
    const submitConversation = document.getElementById('submitConversation');
    const enableDesktopAudio = document.getElementById('enableDesktopAudio');
    const skipDesktopAudio = document.getElementById('skipDesktopAudio');
    const desktopAudioStatus = document.getElementById('desktopAudioStatus');

    // Get follow-up ID from URL
    const urlParams = new URLSearchParams(window.location.search);
    const followupId = urlParams.get('id');

    if (!followupId) {
      alert('Invalid follow-up ID');
      window.location.href = '/';
    }

    // Load voices for better TTS
    function loadVoices() {
      return new Promise((resolve) => {
        if (speechSynthesis.getVoices().length > 0) {
          resolve(speechSynthesis.getVoices());
        } else {
          speechSynthesis.onvoiceschanged = () => {
            resolve(speechSynthesis.getVoices());
          };
        }
      });
    }

    // Initialize page
    async function initialize() {
      await loadTopics();
      await setupCamera();
      await loadVoices();
    }

    // Load topics from follow-up query
    async function loadTopics() {
      try {
        const response = await fetch(`/api/follow-up-queries/${followupId}`, {
          credentials: 'include'
        });

        if (!response.ok) {
          throw new Error('Failed to load topics');
        }

        const data = await response.json();
        const questions = data.questions || [];

        topicsContainer.innerHTML = questions.map((question, index) => `
          <div class="bg-[#1E1E21] border border-white/10 rounded-lg p-3">
            <div class="flex items-start gap-2">
              <span class="bg-accent text-[#1E1E21] text-xs font-bold px-1.5 py-0.5 rounded-full flex-shrink-0">${index + 1}</span>
              <p class="text-white text-sm leading-relaxed">${question}</p>
            </div>
          </div>
        `).join('');
      } catch (error) {
        console.error('Error loading topics:', error);
        topicsContainer.innerHTML = '<p class="text-white/60">Failed to load topics</p>';
      }
    }

    // Setup camera and audio devices
    async function setupCamera() {
      try {
        // Request permissions first to get proper device labels
        await navigator.mediaDevices.getUserMedia({ video: true, audio: true });
        
        const devices = await navigator.mediaDevices.enumerateDevices();
        const videoDevices = devices.filter(device => device.kind === 'videoinput');
        const audioDevices = devices.filter(device => device.kind === 'audioinput');

        console.log('Available video devices:', videoDevices);
        console.log('Available audio devices:', audioDevices);

        // Setup camera dropdown
        cameraSelect.innerHTML = '<option value="">Select Camera...</option>';
        videoDevices.forEach((device, index) => {
          const option = document.createElement('option');
          option.value = device.deviceId;
          const deviceName = device.label || `Camera ${index + 1}`;
          option.textContent = deviceName;
          cameraSelect.appendChild(option);
        });

        // Setup audio dropdown
        audioSelect.innerHTML = '<option value="">Select Audio Device...</option>';
        audioDevices.forEach((device, index) => {
          const option = document.createElement('option');
          option.value = device.deviceId;
          const deviceName = device.label || `Audio Device ${index + 1}`;
          option.textContent = deviceName;
          audioSelect.appendChild(option);
        });

        // Auto-select first devices if available
        if (videoDevices.length > 0) {
          cameraSelect.value = videoDevices[0].deviceId;
        }
        if (audioDevices.length > 0) {
          audioSelect.value = audioDevices[0].deviceId;
        }

        // Start camera if we have a video device selected
        if (videoDevices.length > 0) {
          await startCamera();
        }

        cameraSelect.addEventListener('change', startCamera);
        audioSelect.addEventListener('change', startCamera);
      } catch (error) {
        console.error('Error setting up devices:', error);
        // Fallback: try without permissions
        try {
          const devices = await navigator.mediaDevices.enumerateDevices();
          const videoDevices = devices.filter(device => device.kind === 'videoinput');
          const audioDevices = devices.filter(device => device.kind === 'audioinput');

          // Setup camera dropdown
          cameraSelect.innerHTML = '<option value="">Select Camera...</option>';
          videoDevices.forEach((device, index) => {
            const option = document.createElement('option');
            option.value = device.deviceId;
            option.textContent = device.label || `Camera ${index + 1}`;
            cameraSelect.appendChild(option);
          });

          // Setup audio dropdown
          audioSelect.innerHTML = '<option value="">Select Audio Device...</option>';
          audioDevices.forEach((device, index) => {
            const option = document.createElement('option');
            option.value = device.deviceId;
            option.textContent = device.label || `Audio Device ${index + 1}`;
            cameraSelect.appendChild(option);
          });

          if (videoDevices.length > 0) {
            cameraSelect.value = videoDevices[0].deviceId;
          }
          if (audioDevices.length > 0) {
            audioSelect.value = audioDevices[0].deviceId;
          }

          cameraSelect.addEventListener('change', startCamera);
          audioSelect.addEventListener('change', startCamera);
        } catch (fallbackError) {
          console.error('Fallback device setup failed:', fallbackError);
        }
      }
    }

    // Enable desktop audio capture
    async function enableDesktopAudioCapture() {
      try {
        enableDesktopAudio.disabled = true;
        enableDesktopAudio.textContent = 'Requesting Permission...';
        desktopAudioStatus.textContent = 'Requesting desktop audio permission...';

        // Check if getDisplayMedia is supported
        if (!navigator.mediaDevices || !navigator.mediaDevices.getDisplayMedia) {
          throw new Error('Screen capture not supported in this browser');
        }

        // Try different audio configurations
        let audioConfig = {
          echoCancellation: false,
          noiseSuppression: false,
          autoGainControl: false
        };

        // First try with explicit audio config
        try {
          desktopAudioStream = await navigator.mediaDevices.getDisplayMedia({
            video: false,
            audio: audioConfig
          });
        } catch (audioError) {
          console.log('Audio config failed, trying with basic audio:', audioError);
          // Try with basic audio
          desktopAudioStream = await navigator.mediaDevices.getDisplayMedia({
            video: false,
            audio: true
          });
        }
        
        // Check if we actually got audio
        const audioTracks = desktopAudioStream.getAudioTracks();
        if (audioTracks.length === 0) {
          throw new Error('No audio tracks available from screen capture');
        }
        
        desktopAudioEnabled = true;
        enableDesktopAudio.textContent = '✓ Desktop Audio Enabled';
        enableDesktopAudio.classList.remove('bg-blue-600', 'hover:bg-blue-700');
        enableDesktopAudio.classList.add('bg-green-600', 'hover:bg-green-700');
        desktopAudioStatus.textContent = 'Desktop audio capture enabled';
        
        console.log('Desktop audio capture successful');
        
        // Re-setup camera with desktop audio
        await startCamera();
        
      } catch (error) {
        console.error('Desktop audio capture failed:', error);
        enableDesktopAudio.disabled = false;
        enableDesktopAudio.textContent = 'Enable Desktop Audio';
        
        // Provide more specific error messages
        let errorMessage = 'Failed to enable desktop audio. ';
        if (error.message.includes('Not supported')) {
          errorMessage += 'Your browser may not support desktop audio capture. ';
        } else if (error.message.includes('Permission denied')) {
          errorMessage += 'Permission was denied. Please try again and allow audio capture. ';
        } else if (error.message.includes('NotAllowedError')) {
          errorMessage += 'Audio capture was blocked. Please check your browser settings. ';
        }
        errorMessage += 'You can still proceed with microphone only.';
        
        desktopAudioStatus.textContent = errorMessage;
        
        // Still allow camera setup without desktop audio
        await startCamera();
      }
    }

    // Skip desktop audio capture
    async function skipDesktopAudioCapture() {
      desktopAudioStatus.textContent = 'Skipped desktop audio - using microphone only';
      enableDesktopAudio.disabled = true;
      skipDesktopAudio.disabled = true;
      
      // Proceed with camera setup without desktop audio
      await startCamera();
    }

    // Start camera with desktop audio capture
    async function startCamera() {
      const videoDeviceId = cameraSelect.value;
      const audioDeviceId = audioSelect.value;
      
      if (!videoDeviceId) return;

      try {
        // Stop existing stream
        if (stream) {
          stream.getTracks().forEach(track => track.stop());
        }

        // First, get user media for camera and microphone
        const userMediaConstraints = {
          video: {
            deviceId: { exact: videoDeviceId },
            width: { ideal: 1280 },
            height: { ideal: 720 },
            facingMode: 'user'
          },
          audio: {
            deviceId: audioDeviceId ? { exact: audioDeviceId } : true,
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true
          }
        };

        const userStream = await navigator.mediaDevices.getUserMedia(userMediaConstraints);
        
        // Combine streams if we have desktop audio
        if (desktopAudioEnabled && desktopAudioStream) {
          // Create a new MediaStream with video from user media and audio from both sources
          const combinedStream = new MediaStream();
          
          // Add video track from user media
          const videoTrack = userStream.getVideoTracks()[0];
          if (videoTrack) {
            combinedStream.addTrack(videoTrack);
          }
          
          // Add microphone audio track
          const micAudioTrack = userStream.getAudioTracks()[0];
          if (micAudioTrack) {
            combinedStream.addTrack(micAudioTrack);
          }
          
          // Add desktop audio track
          const desktopAudioTrack = desktopAudioStream.getAudioTracks()[0];
          if (desktopAudioTrack) {
            combinedStream.addTrack(desktopAudioTrack);
          }
          
          stream = combinedStream;
        } else {
          // Use just the user media stream
          stream = userStream;
        }

        currentStream = stream;

        videoPreview.srcObject = stream;
        videoPreview.classList.remove('hidden');
        videoPlaceholder.classList.add('hidden');
        
        // Enable start conversation button
        startConversation.disabled = false;
        
        // Setup audio level monitoring
        setupAudioLevelMonitoring(stream);
        
        // Show success message for desktop audio
        if (desktopAudioEnabled) {
          console.log('Desktop audio capture enabled - AI voice will be recorded');
        }
        
      } catch (error) {
        console.error('Error starting camera:', error);
        Swal.fire({
          icon: 'error',
          title: 'Camera Access Failed',
          text: 'Failed to access camera. Please check permissions and try again.',
          confirmButtonColor: '#f1d85b',
          confirmButtonText: 'OK'
        });
      }
    }

    // Show AI speaking indicator
    function showAISpeaking() {
      isAISpeaking = true;
      aiSpeakingIndicator.classList.remove('hidden');
      aiStatusDisplay.classList.remove('hidden');
      aiStatusText.textContent = 'AI Assistant is speaking...';
    }

    // Hide AI speaking indicator
    function hideAISpeaking() {
      isAISpeaking = false;
      aiSpeakingIndicator.classList.add('hidden');
      aiStatusDisplay.classList.add('hidden');
    }

    // Stop any current speech
    function stopCurrentSpeech() {
      if (currentSpeech) {
        speechSynthesis.cancel();
        currentSpeech = null;
        hideAISpeaking();
      }
    }

    // Generate and play AI audio using Web Speech API
    async function playAIAudio(text) {
      try {
        // Stop any current speech first
        stopCurrentSpeech();
        
        // Use Web Speech API as fallback
        if ('speechSynthesis' in window) {
          const utterance = new SpeechSynthesisUtterance(text);
          utterance.rate = 0.85; // Slightly slower for better clarity
          utterance.pitch = 1.1; // Slightly higher pitch for female voice
          utterance.volume = 0.9;
          
          // Try to use a better female voice if available
          const voices = speechSynthesis.getVoices();
          const preferredVoice = voices.find(voice => 
            voice.name.includes('Samantha') ||
            voice.name.includes('Karen') ||
            voice.name.includes('Victoria') ||
            voice.name.includes('Susan') ||
            voice.name.includes('Google UK Female') ||
            voice.name.includes('Microsoft Zira') ||
            voice.name.includes('Microsoft Hazel') ||
            voice.name.includes('Google US Female') ||
            (voice.name.includes('Female') && voice.lang.startsWith('en'))
          );
          
          if (preferredVoice) {
            utterance.voice = preferredVoice;
            console.log('Using voice:', preferredVoice.name);
          }
          
          utterance.onend = () => {
            currentSpeech = null;
            hideAISpeaking();
          };
          
          utterance.onerror = () => {
            console.error('Speech synthesis error');
            currentSpeech = null;
            hideAISpeaking();
          };
          
          currentSpeech = utterance;
          speechSynthesis.speak(utterance);
        } else {
          // Fallback: just hide the speaking indicator after a delay
          console.log('AI Speaking:', text);
          setTimeout(() => {
            hideAISpeaking();
          }, text.length * 50); // Rough estimate: 50ms per character
        }
      } catch (error) {
        console.error('Error playing AI audio:', error);
        hideAISpeaking();
      }
    }

    // Start AI conversation
    async function startAIConversation() {
      if (!stream) return;

      // Get topics from the container
      const topicElements = topicsContainer.querySelectorAll('.bg-\\[\\#1E1E21\\]');
      const topics = Array.from(topicElements).map(el => 
        el.querySelector('p').textContent.trim()
      );

      if (topics.length === 0) {
        Swal.fire({
          icon: 'error',
          title: 'No Topics',
          text: 'No topics found. Please refresh the page.',
          confirmButtonColor: '#f1d85b',
          confirmButtonText: 'OK'
        });
        return;
      }

      // Hide start section and show recording controls
      startConversationSection.classList.add('hidden');
      recordingControls.classList.remove('hidden');
      bottomDock.classList.remove('hidden');

      // Start recording
      startRecordingVideo();
      
      // Start AI conversation
      conversationStarted = true;
      await startGeminiConversation(topics);
    }

    // Start Gemini conversation
    async function startGeminiConversation(topics) {
      try {
        // Create conversation prompt with topics
        const topicsText = topics.map((topic, i) => `${i + 1}. ${topic}`).join('\n');
        const systemPrompt = `You are a friendly, professional AI assistant conducting a natural conversation. 

Your goal is to have a natural, flowing conversation that covers these topics:
${topicsText}

Guidelines:
- Be conversational and friendly, not interrogative
- Ask follow-up questions naturally
- Don't rush through topics - let the conversation flow
- Be encouraging and supportive
- If the person seems to finish a topic, gently guide to the next one
- Keep the conversation engaging and comfortable

Start by greeting the person warmly and begin the conversation naturally.`;

        // Use text-to-speech for AI conversation
        console.log('Starting AI conversation with TTS');
        await startAIConversationFlow(topics);
        
      } catch (error) {
        console.error('Error starting Gemini conversation:', error);
        // Fallback to simple TTS
        showAISpeaking();
        await playAIAudio("Hello! Let's have a great conversation. I'll ask you some questions naturally as we go. Please feel free to share your thoughts openly.");
      }
    }

    // Note: Gemini Live API integration removed due to CSP issues
    // The system now uses TTS for AI responses, which works reliably

    // Start AI conversation flow with TTS
    async function startAIConversationFlow(topics) {
      try {
        // Welcome message
        showAISpeaking();
        await playAIAudio("Hello! I'm excited to have this conversation with you. I'll be asking you some questions naturally as we go. Please feel free to share your thoughts openly and take your time with your responses.");
        
        // Wait a moment before starting the conversation
        await new Promise(resolve => setTimeout(resolve, 2000));
        
        // Start asking about topics naturally
        for (let i = 0; i < topics.length; i++) {
          const topic = topics[i];
          
          // Create a natural conversation flow
          const conversationPrompts = [
            `Let's talk about ${topic.toLowerCase()}. I'd love to hear your thoughts on this.`,
            `I'm curious about your experience with ${topic.toLowerCase()}. Could you share some insights?`,
            `Tell me about ${topic.toLowerCase()}. What's your perspective on this?`,
            `I'd like to understand more about ${topic.toLowerCase()}. What can you tell me?`
          ];
          
          const prompt = conversationPrompts[i % conversationPrompts.length];
          
          showAISpeaking();
          await playAIAudio(prompt);
          
          // Wait for user to respond (simulate conversation timing)
          await new Promise(resolve => setTimeout(resolve, 15000)); // 15 seconds per topic
        }
        
        // Closing message
        showAISpeaking();
        await playAIAudio("Thank you for sharing your insights with me. That was a great conversation! Feel free to add anything else you'd like to mention, or we can wrap up here.");
        
      } catch (error) {
        console.error('Error in AI conversation flow:', error);
        // Fallback message
        showAISpeaking();
        await playAIAudio("Thank you for the conversation! Please feel free to share any additional thoughts you have.");
      }
    }

    // Start recording
    function startRecordingVideo() {
      if (!stream) return;

      recordedChunks = [];
      mediaRecorder = new MediaRecorder(stream, {
        mimeType: 'video/webm;codecs=vp9,opus'
      });

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          recordedChunks.push(event.data);
        }
      };

      mediaRecorder.onstop = () => {
        const blob = new Blob(recordedChunks, { type: 'video/webm' });
        const url = URL.createObjectURL(blob);
        recordedVideoBlob = blob;
        
        // Update video preview to show recorded video
        videoPreview.src = url;
        videoPreview.srcObject = null;
        videoPreview.muted = false;
      };

      mediaRecorder.start();
      recordingStartTime = Date.now();
      startRecordingTimer();
      isRecording = true;
      
      // Audio capture is handled by the MediaRecorder
      // Desktop audio capture ensures AI voice is recorded
    }

    // Audio processing functions removed - using TTS instead of Live API

    // Stop recording
    function stopRecordingVideo() {
      if (mediaRecorder && mediaRecorder.state === 'recording') {
        mediaRecorder.stop();
        clearInterval(recordingTimer);
        isRecording = false;
        
        // Stop any current AI speech
        stopCurrentSpeech();
        
        // Clean up any remaining resources
        // (Gemini Live API cleanup removed)
      }
    }

    // Start recording timer
    function startRecordingTimer() {
      recordingTimer = setInterval(() => {
        const elapsed = Date.now() - recordingStartTime;
        const minutes = Math.floor(elapsed / 60000);
        const seconds = Math.floor((elapsed % 60000) / 1000);
        recordingTime.textContent = `${minutes.toString().padStart(2, '0')}:${seconds.toString().padStart(2, '0')}`;
      }, 1000);
    }

    // Setup audio level monitoring
    function setupAudioLevelMonitoring(stream) {
      try {
        audioContext = new (window.AudioContext || window.webkitAudioContext)();
        analyser = audioContext.createAnalyser();
        microphone = audioContext.createMediaStreamSource(stream);
        
        analyser.fftSize = 256;
        microphone.connect(analyser);
        
        const audioLevelIndicator = document.getElementById('audioLevelIndicator');
        audioLevelIndicator.classList.remove('hidden');
        
        const dataArray = new Uint8Array(analyser.frequencyBinCount);
        
        function updateAudioLevel() {
          analyser.getByteFrequencyData(dataArray);
          
          // Calculate average volume
          let sum = 0;
          for (let i = 0; i < dataArray.length; i++) {
            sum += dataArray[i];
          }
          const average = sum / dataArray.length;
          const normalizedLevel = average / 255;
          
          // Update visual bars
          const bars = ['audioBar1', 'audioBar2', 'audioBar3', 'audioBar4', 'audioBar5'];
          bars.forEach((barId, index) => {
            const bar = document.getElementById(barId);
            const threshold = (index + 1) / 5;
            if (normalizedLevel >= threshold) {
              bar.style.backgroundColor = normalizedLevel > 0.8 ? '#ef4444' : '#10b981';
            } else {
              bar.style.backgroundColor = '#374151';
            }
          });
          
          animationFrame = requestAnimationFrame(updateAudioLevel);
        }
        
        updateAudioLevel();
      } catch (error) {
        console.error('Error setting up audio monitoring:', error);
      }
    }

    // Restart recording function
    function restartRecordingFunction() {
      // Stop current recording
      if (isRecording) {
        stopRecordingVideo();
      }
      
      // Reset UI
      startConversationSection.classList.remove('hidden');
      recordingControls.classList.add('hidden');
      bottomDock.classList.add('hidden');
      
      // Reset video preview to show camera feed
      if (currentStream) {
        videoPreview.srcObject = currentStream;
        videoPreview.muted = true;
        videoPreview.classList.remove('hidden');
        videoPlaceholder.classList.add('hidden');
      }
      
      // Stop audio monitoring
      if (audioContext) {
        audioContext.close();
        audioContext = null;
      }
      if (animationFrame) {
        cancelAnimationFrame(animationFrame);
      }
      document.getElementById('audioLevelIndicator').classList.add('hidden');
      
      conversationStarted = false;
    }

    // Submit conversation
    async function submitConversationFunction() {
      if (!recordedVideoBlob) {
        Swal.fire({
          icon: 'error',
          title: 'No Video',
          text: 'No recorded conversation found. Please record a conversation first.',
          confirmButtonColor: '#f1d85b',
          confirmButtonText: 'OK'
        });
        return;
      }

      const formData = new FormData();
      formData.append('video', recordedVideoBlob, 'conversation-video.webm');
      formData.append('followupId', followupId);

      // Hide controls and show upload progress
      bottomDock.classList.add('hidden');
      recordingControls.classList.add('hidden');
      uploadProgress.classList.remove('hidden');

      try {
        const response = await fetch('/api/follow-up-queries/submit-video', {
          method: 'POST',
          body: formData,
          credentials: 'include'
        });

        if (!response.ok) {
          throw new Error('Failed to submit video');
        }

        // Hide upload progress and show processing loader
        uploadProgress.classList.add('hidden');
        processingLoader.classList.remove('hidden');

        const data = await response.json();
        
        // Check if the response indicates success
        if (data.success && data.message) {
          // Hide processing loader and show completion message
          processingLoader.classList.add('hidden');
          completionMessage.classList.remove('hidden');
        } else {
          // Handle unexpected response format
          throw new Error(data.error || 'Unexpected response format');
        }
        
      } catch (error) {
        console.error('Error submitting video:', error);
        
        // Determine error message based on the error
        let errorTitle = 'Upload Failed';
        let errorText = 'Failed to submit conversation. Please try again.';
        
        if (error.message.includes('parse Gemini response')) {
          errorTitle = 'Processing Failed';
          errorText = 'The conversation was uploaded but failed to process with AI. Please try again.';
        } else if (error.message.includes('Failed to submit video')) {
          errorTitle = 'Upload Failed';
          errorText = 'Failed to upload conversation to server. Please check your connection and try again.';
        }
        
        Swal.fire({
          icon: 'error',
          title: errorTitle,
          text: errorText,
          confirmButtonColor: '#f1d85b',
          confirmButtonText: 'OK'
        });
        
        uploadProgress.classList.add('hidden');
        processingLoader.classList.add('hidden');
        bottomDock.classList.remove('hidden');
        recordingControls.classList.remove('hidden');
      }
    }

    // Close tab
    function closeTab() {
      // Try multiple methods to close the tab
      try {
        // Method 1: Standard window.close()
        window.close();
        
        // Method 2: Try to close via history
        if (window.history.length > 1) {
          window.history.back();
        }
        
        // Method 3: Try to navigate away
        window.location.href = 'about:blank';
        
        // If none of the above work, show instructions after a short delay
        setTimeout(() => {
          // Check if we're still on the same page
          if (window.location.href.includes('ai-chat.html')) {
            showCloseInstructions();
          }
        }, 200);
        
      } catch (error) {
        console.log('Auto-close failed, showing manual instructions');
        showCloseInstructions();
      }
    }

    // Show close instructions
    function showCloseInstructions() {
      Swal.fire({
        icon: 'success',
        title: 'Conversation Completed Successfully!',
        html: `
          <div class="text-left space-y-3">
            <p class="text-gray-700">Your conversation has been processed and analyzed. The transcript and insights have been saved.</p>
            <p class="text-gray-700 font-medium">To close this tab, you can:</p>
            <div class="space-y-2">
              <div class="flex items-center gap-2">
                <span class="bg-gray-200 px-2 py-1 rounded text-sm font-mono">Ctrl + W</span>
                <span class="text-sm text-gray-600">(Windows/Linux)</span>
              </div>
              <div class="flex items-center gap-2">
                <span class="bg-gray-200 px-2 py-1 rounded text-sm font-mono">Cmd + W</span>
                <span class="text-sm text-gray-600">(Mac)</span>
              </div>
              <div class="flex items-center gap-2">
                <span class="bg-gray-200 px-2 py-1 rounded text-sm font-mono">Click the X</span>
                <span class="text-sm text-gray-600">(On the tab)</span>
              </div>
            </div>
            <p class="text-sm text-gray-600 mt-3">Thank you for completing the conversation!</p>
          </div>
        `,
        confirmButtonColor: '#f1d85b',
        confirmButtonText: 'Got it!',
        allowOutsideClick: false,
        allowEscapeKey: false
      });
    }

    // Event listeners
    startConversation.addEventListener('click', startAIConversation);
    restartRecording.addEventListener('click', restartRecordingFunction);
    submitConversation.addEventListener('click', submitConversationFunction);
    enableDesktopAudio.addEventListener('click', enableDesktopAudioCapture);
    skipDesktopAudio.addEventListener('click', skipDesktopAudioCapture);

    // Initialize when page loads
    document.addEventListener('DOMContentLoaded', initialize);
  </script>
</body>
</html>